#!/usr/bin/env npx tsx
/**
 * Track B: SEO Auto-Pipeline
 * 
 * Extracts trending keywords from daily news data (news_items table)
 * and auto-generates long-tail content (T2/T3/FAQ/Glossary/Compare).
 * 
 * Usage: npx tsx scripts/seo-pipeline.ts [--dry-run] [--date YYYY-MM-DD] [--limit N]
 * 
 * Requires: ANTHROPIC_API_KEY or claude CLI available
 * Optional: BRAVE_API_KEY for search demand validation
 */

import * as fs from 'fs';
import * as path from 'path';
import { execSync, spawn } from 'child_process';
import * as dotenv from 'dotenv';

dotenv.config();

// â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const PROJECT_ROOT = process.cwd();
const OUTPUT_DIR = path.join(PROJECT_ROOT, 'output', 'seo-pipeline');
const CONTENT_DIRS = {
  glossary: path.join(PROJECT_ROOT, 'content', 'glossary'),
  faq: path.join(PROJECT_ROOT, 'content', 'faq'),
  compare: path.join(PROJECT_ROOT, 'content', 'compare'),
  tier2: path.join(PROJECT_ROOT, 'content', 'blogs'),
  tier3: path.join(PROJECT_ROOT, 'content', 'blogs'),
};
const DB_PATH = path.join(PROJECT_ROOT, 'data', 'loreai.db');
const BRAVE_API_KEY = process.env.BRAVE_API_KEY;

// â”€â”€â”€ CLI Args â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const args = process.argv.slice(2);
const DRY_RUN = args.includes('--dry-run');
const dateIdx = args.indexOf('--date');
const TARGET_DATE = dateIdx >= 0 ? args[dateIdx + 1] : new Date().toISOString().split('T')[0];
const limitIdx = args.indexOf('--limit');
const MAX_ITEMS = limitIdx >= 0 ? parseInt(args[limitIdx + 1]) : 10;

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface NewsItem {
  id: string;
  title: string;
  url: string;
  source: string;
  source_tier: number;
  category: string;
  score: number;
  raw_summary: string;
  detected_at: string;
}

interface ExtractedKeyword {
  keyword: string;
  keyword_zh: string;
  type: 'glossary' | 'compare' | 'faq' | 'tier2' | 'tier3';
  relevance: number;  // 1-10
  newness: number;     // 1-10
  context: string;     // brief explanation
  news_ids: string[];  // related news_items
}

interface GeneratedContent {
  keyword: ExtractedKeyword;
  slug: string;
  en_path: string;
  zh_path: string;
  status: 'success' | 'error';
  error?: string;
}

// â”€â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function log(step: string, msg: string) {
  const ts = new Date().toISOString().split('T')[1].split('.')[0];
  console.log(`[${ts}] [${step}] ${msg}`);
}

// â”€â”€â”€ Database â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function getNewsItems(): NewsItem[] {
  try {
    // Use better-sqlite3 via dynamic import workaround
    const result = execSync(
      `npx tsx -e "
        import Database from 'better-sqlite3';
        const db = new Database('${DB_PATH}');
        const rows = db.prepare(\\"SELECT * FROM news_items WHERE detected_at > datetime('now', '-48 hours') ORDER BY score DESC, detected_at DESC LIMIT 100\\").all();
        console.log(JSON.stringify(rows));
        db.close();
      "`,
      { cwd: PROJECT_ROOT, timeout: 30000, encoding: 'utf-8' }
    );
    return JSON.parse(result.trim());
  } catch (e) {
    log('DB', `Error reading news_items: ${(e as Error).message}`);
    return [];
  }
}

// â”€â”€â”€ Existing Content Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function getExistingSlugs(): Set<string> {
  const slugs = new Set<string>();
  const dirs = [
    'content/glossary', 'content/faq', 'content/compare',
    'content/blogs/en', 'content/blogs/zh'
  ];
  for (const dir of dirs) {
    const fullPath = path.join(PROJECT_ROOT, dir);
    if (!fs.existsSync(fullPath)) continue;
    for (const file of fs.readdirSync(fullPath)) {
      if (file.endsWith('.md')) {
        slugs.add(file.replace(/-(en|zh)\.md$/, '').replace(/\.md$/, ''));
      }
    }
  }
  return slugs;
}

function getExistingTitles(): Set<string> {
  const titles = new Set<string>();
  const dirs = [
    'content/glossary', 'content/faq', 'content/compare',
    'content/blogs/en', 'content/blogs/zh'
  ];
  for (const dir of dirs) {
    const fullPath = path.join(PROJECT_ROOT, dir);
    if (!fs.existsSync(fullPath)) continue;
    for (const file of fs.readdirSync(fullPath)) {
      if (!file.endsWith('.md')) continue;
      try {
        const content = fs.readFileSync(path.join(fullPath, file), 'utf-8');
        const titleMatch = content.match(/^title:\s*["']?(.+?)["']?\s*$/m);
        if (titleMatch) titles.add(titleMatch[1].toLowerCase());
      } catch {}
    }
  }
  return titles;
}

// â”€â”€â”€ Claude CLI Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function runClaudePipe(prompt: string, timeoutMs = 5 * 60 * 1000): string {
  const tmpFile = path.join('/tmp', `seo-pipeline-${Date.now()}.txt`);
  fs.writeFileSync(tmpFile, prompt);
  try {
    const output = execSync(
      `cat "${tmpFile}" | claude -p --allowedTools "" 2>/dev/null`,
      { cwd: PROJECT_ROOT, timeout: timeoutMs, encoding: 'utf-8', maxBuffer: 10 * 1024 * 1024 }
    );
    return output.trim();
  } finally {
    try { fs.unlinkSync(tmpFile); } catch {}
  }
}

// â”€â”€â”€ Step 1: Keyword Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function extractKeywords(newsItems: NewsItem[], existingSlugs: Set<string>, existingTitles: Set<string>): Promise<ExtractedKeyword[]> {
  log('KEYWORDS', `Extracting from ${newsItems.length} news items...`);

  const newsContext = newsItems.map((n, i) => 
    `[${i + 1}] ${n.title} (${n.source}, score: ${n.score})\n    ${n.raw_summary || ''}\n    ID: ${n.id}`
  ).join('\n\n');

  const existingList = [...existingSlugs].slice(0, 100).join(', ');

  const prompt = `You are an SEO keyword extraction specialist for LoreAI (loreai.dev), an AI industry news and analysis site targeting AI developers, engineers, and product managers.

Given these recent AI news items (last 48 hours), extract 10-15 trending keywords/topics that would make good long-tail SEO content.

## News Items

${newsContext}

## Already Existing Content (DO NOT duplicate these)

${existingList}

## Output Requirements

Return ONLY a valid JSON array. Each item:
{
  "keyword": "English keyword/phrase",
  "keyword_zh": "ä¸­æ–‡å…³é”®è¯",
  "type": "glossary|compare|faq|tier2|tier3",
  "relevance": 1-10,
  "newness": 1-10,
  "context": "Brief explanation of why this keyword matters now",
  "news_ids": ["id1", "id2"],
  "category": "models|tools|infra|safety|opensource|applications"
}

## MANDATORY: Diversity Requirements

You MUST cover at least 4 different categories and at least 3 different companies/organizations. Do NOT focus on a single vendor.

### Category Quotas (minimum):
- **models** (2-3): New model releases, benchmarks, architecture innovations (from ANY vendor: OpenAI, Google, Meta, Mistral, Chinese labs, startups)
- **tools** (2-3): Developer tools, APIs, SDKs, frameworks, IDE integrations
- **infra** (1-2): Infrastructure, deployment, cost optimization, MLOps
- **opensource** (1-2): Open-source models, datasets, community projects on HuggingFace etc.
- **applications** (1-2): Real-world AI use cases, industry adoption, practical implementations
- **safety** (0-1): AI safety, alignment, policy, responsible AI

### Type Assignment Rules
- New concept/term that needs definition â†’ "glossary"
- X vs Y natural comparison â†’ "compare"  
- Common questions people would search â†’ "faq"
- Trend analysis requiring depth â†’ "tier2"
- Practical how-to or quick explainer â†’ "tier3"

## Scoring
- relevance: How relevant to AI developers/PMs (our audience)
- newness: How new/trending (10 = breaking, 1 = well-known)

## Rules
- CRITICAL: Do NOT make more than 5 keywords about the same company/product
- Skip keywords that match existing content slugs
- Each keyword should have real news data backing it
- Prefer actionable, searchable phrases over vague topics
- Mix of types (don't make all glossary or all FAQ)
- Include both technical and practical angles
- Think about what an AI engineer would actually Google right now

Return ONLY the JSON array, no markdown fences, no explanation.`;

  const output = runClaudePipe(prompt);
  
  try {
    // Extract JSON from output (handle markdown fences if present)
    const jsonMatch = output.match(/\[[\s\S]*\]/);
    if (!jsonMatch) throw new Error('No JSON array found in output');
    const keywords: ExtractedKeyword[] = JSON.parse(jsonMatch[0]);
    
    // Filter out duplicates
    const filtered = keywords.filter(k => {
      const slug = k.keyword.toLowerCase().replace(/[^a-z0-9]+/g, '-').replace(/^-|-$/g, '');
      if (existingSlugs.has(slug)) {
        log('KEYWORDS', `  Skipping duplicate: ${k.keyword}`);
        return false;
      }
      if (existingTitles.has(k.keyword.toLowerCase())) {
        log('KEYWORDS', `  Skipping duplicate title: ${k.keyword}`);
        return false;
      }
      return true;
    });

    // Sort by combined score
    filtered.sort((a, b) => (b.relevance * b.newness) - (a.relevance * a.newness));

    log('KEYWORDS', `Extracted ${filtered.length} keywords (filtered from ${keywords.length})`);
    return filtered.slice(0, MAX_ITEMS);
  } catch (e) {
    log('KEYWORDS', `Parse error: ${(e as Error).message}`);
    log('KEYWORDS', `Raw output: ${output.slice(0, 500)}`);
    return [];
  }
}

// â”€â”€â”€ Step 2: Brave Search Validation (Optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function validateWithBrave(keywords: ExtractedKeyword[]): Promise<ExtractedKeyword[]> {
  if (!BRAVE_API_KEY) {
    log('BRAVE', 'No BRAVE_API_KEY set, skipping search validation');
    return keywords;
  }

  log('BRAVE', `Validating ${keywords.length} keywords against Brave Search...`);
  const validated: ExtractedKeyword[] = [];

  for (const kw of keywords) {
    try {
      const url = `https://api.search.brave.com/res/v1/web/search?q=${encodeURIComponent(kw.keyword)}&count=5`;
      const resp = await fetch(url, {
        headers: { 'X-Subscription-Token': BRAVE_API_KEY, 'Accept': 'application/json' }
      });
      if (!resp.ok) {
        log('BRAVE', `  API error for "${kw.keyword}": ${resp.status}`);
        validated.push(kw); // Pass through on error
        continue;
      }
      const data = await resp.json() as any;
      const resultCount = data?.web?.results?.length || 0;
      if (resultCount > 0) {
        log('BRAVE', `  âœ… "${kw.keyword}" â€” ${resultCount} results`);
        validated.push(kw);
      } else {
        log('BRAVE', `  âŒ "${kw.keyword}" â€” no results, skipping`);
      }
      // Rate limit
      await new Promise(r => setTimeout(r, 500));
    } catch (e) {
      log('BRAVE', `  Error checking "${kw.keyword}": ${(e as Error).message}`);
      validated.push(kw); // Pass through on error
    }
  }

  log('BRAVE', `${validated.length}/${keywords.length} keywords validated`);
  return validated;
}

// â”€â”€â”€ Step 3: Content Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function generateContent(keyword: ExtractedKeyword, newsItems: NewsItem[]): GeneratedContent {
  const slug = keyword.keyword.toLowerCase().replace(/[^a-z0-9]+/g, '-').replace(/^-|-$/g, '');
  const date = TARGET_DATE;
  const relatedNews = newsItems
    .filter(n => keyword.news_ids.includes(n.id))
    .map(n => `- ${n.title} (${n.source})\n  ${n.raw_summary || ''}`)
    .join('\n');

  const result: GeneratedContent = {
    keyword,
    slug,
    en_path: '',
    zh_path: '',
    status: 'success',
  };

  try {
    switch (keyword.type) {
      case 'glossary':
        generateGlossary(keyword, slug, date, relatedNews, result);
        break;
      case 'faq':
        generateFAQ(keyword, slug, date, relatedNews, result);
        break;
      case 'compare':
        generateCompare(keyword, slug, date, relatedNews, result);
        break;
      case 'tier2':
        generateTier2(keyword, slug, date, relatedNews, result);
        break;
      case 'tier3':
        generateTier3(keyword, slug, date, relatedNews, result);
        break;
    }
  } catch (e) {
    result.status = 'error';
    result.error = (e as Error).message;
    log('GENERATE', `âŒ Error generating ${keyword.type} for "${keyword.keyword}": ${result.error}`);
  }

  return result;
}

function generateGlossary(kw: ExtractedKeyword, slug: string, date: string, news: string, result: GeneratedContent) {
  log('GENERATE', `ðŸ“– Glossary: ${kw.keyword}`);
  
  const enPrompt = `Write a glossary entry for the AI term "${kw.keyword}" for loreai.dev.

## Context from recent news
${news || 'No specific news context available.'}

## Format
Output ONLY the complete markdown file with frontmatter. No explanations before or after.

---
slug: ${slug}
title: "${kw.keyword} â€” What It Is and Why It Matters"
description: "Learn what ${kw.keyword} means in AI, how it works, and why it matters for developers and businesses."
keywords: ["${kw.keyword}", "AI glossary", "AI terminology"]
date: ${date}
tier: 3
lang: en
type: glossary
tags: ["glossary", "AI"]
---

# ${kw.keyword}

## Definition
(Clear 2-3 sentence definition)

## Why It Matters
(2-3 paragraphs on relevance, especially given recent developments)

## How It Works
(Technical explanation, accessible to developers)

## Related Terms
(List 3-5 related terms with brief definitions)

## Further Reading
(Reference the news sources if relevant)

Requirements:
- 300-500 words total
- Professional but accessible tone
- Include recent context from the news items
- No fluff phrases like "Let's dive in" or "In this article"`;

  const zhPrompt = `ä¸º loreai.dev æ’°å†™ AI æœ¯è¯­"${kw.keyword_zh}"ï¼ˆ${kw.keyword}ï¼‰çš„æœ¯è¯­è¡¨æ¡ç›®ã€‚

## æœ€è¿‘ç›¸å…³æ–°é—»
${news || 'æš‚æ— å…·ä½“æ–°é—»ä¸Šä¸‹æ–‡ã€‚'}

## æ ¼å¼è¦æ±‚
åªè¾“å‡ºå®Œæ•´çš„ markdown æ–‡ä»¶ï¼ˆå« frontmatterï¼‰ï¼Œå‰åŽä¸è¦ä»»ä½•è§£é‡Šã€‚

---
slug: ${slug}
title: "${kw.keyword_zh}ï¼ˆ${kw.keyword}ï¼‰â€” å®šä¹‰ã€åŽŸç†ä¸Žåº”ç”¨"
description: "äº†è§£${kw.keyword_zh}çš„å«ä¹‰ã€å·¥ä½œåŽŸç†ä»¥åŠå¯¹å¼€å‘è€…å’Œä¼ä¸šçš„é‡è¦æ€§ã€‚"
keywords: ["${kw.keyword_zh}", "${kw.keyword}", "AIæœ¯è¯­"]
date: ${date}
tier: 3
lang: zh
type: glossary
tags: ["æœ¯è¯­è¡¨", "AI"]
---

# ${kw.keyword_zh}ï¼ˆ${kw.keyword}ï¼‰

## å®šä¹‰
ï¼ˆæ¸…æ™°çš„2-3å¥å®šä¹‰ï¼‰

## ä¸ºä»€ä¹ˆé‡è¦
ï¼ˆ2-3æ®µï¼Œç»“åˆæœ€æ–°åŠ¨æ€è¯´æ˜Žé‡è¦æ€§ï¼‰

## å·¥ä½œåŽŸç†
ï¼ˆæŠ€æœ¯è§£é‡Šï¼Œå¼€å‘è€…å¯ç†è§£çš„æ·±åº¦ï¼‰

## ç›¸å…³æœ¯è¯­
ï¼ˆåˆ—å‡º3-5ä¸ªç›¸å…³æœ¯è¯­åŠç®€è¦å®šä¹‰ï¼‰

## å»¶ä¼¸é˜…è¯»
ï¼ˆå¼•ç”¨æ–°é—»æ¥æºï¼‰

è¦æ±‚ï¼š
- 300-500å­—
- ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­æ°”
- ä¸“ä¸šæœ¯è¯­é¦–æ¬¡å‡ºçŽ°æ ‡æ³¨è‹±æ–‡ï¼šå¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰
- ä¸è¦ç¿»è¯‘è…”ï¼ˆ"å€¼å¾—æ³¨æ„çš„æ˜¯"ã€"è®©æˆ‘ä»¬æ¥çœ‹çœ‹"ï¼‰`;

  const enContent = runClaudePipe(enPrompt);
  const zhContent = runClaudePipe(zhPrompt);

  const enPath = DRY_RUN 
    ? path.join(OUTPUT_DIR, `glossary-${slug}-en.md`)
    : path.join(CONTENT_DIRS.glossary, `${slug}-en.md`);
  const zhPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `glossary-${slug}-zh.md`)
    : path.join(CONTENT_DIRS.glossary, `${slug}-zh.md`);

  fs.writeFileSync(enPath, extractMarkdown(enContent));
  fs.writeFileSync(zhPath, extractMarkdown(zhContent));
  result.en_path = enPath;
  result.zh_path = zhPath;
}

function generateFAQ(kw: ExtractedKeyword, slug: string, date: string, news: string, result: GeneratedContent) {
  log('GENERATE', `â“ FAQ: ${kw.keyword}`);

  const enPrompt = `Write a FAQ page about "${kw.keyword}" for loreai.dev (AI industry site).

## Context from recent news
${news || 'No specific news context.'}

## Format
Output ONLY the complete markdown file with frontmatter.

---
slug: ${slug}
title: "${kw.keyword} â€” Frequently Asked Questions"
description: "Answers to the most common questions about ${kw.keyword} in AI."
keywords: ["${kw.keyword}", "${kw.keyword} FAQ", "AI FAQ"]
date: ${date}
tier: 3
lang: en
type: faq
tags: ["faq", "AI"]
---

# ${kw.keyword}: Frequently Asked Questions

Generate 10 Q&A pairs. Requirements:
- Questions should be scenario-based (what real users would search)
- Answers: 100-300 words each
- Use H3 (###) for each question
- Include practical examples where relevant
- Reference recent news developments
- No fluff phrases`;

  const zhPrompt = `ä¸º loreai.dev æ’°å†™å…³äºŽ"${kw.keyword_zh}"ï¼ˆ${kw.keyword}ï¼‰çš„å¸¸è§é—®é¢˜é¡µé¢ã€‚

## æœ€è¿‘ç›¸å…³æ–°é—»
${news || 'æš‚æ— å…·ä½“æ–°é—»ä¸Šä¸‹æ–‡ã€‚'}

## æ ¼å¼è¦æ±‚
åªè¾“å‡ºå®Œæ•´çš„ markdown æ–‡ä»¶ï¼ˆå« frontmatterï¼‰ã€‚

---
slug: ${slug}
title: "${kw.keyword_zh} â€” å¸¸è§é—®é¢˜è§£ç­”"
description: "å…³äºŽ${kw.keyword_zh}çš„å¸¸è§é—®é¢˜ä¸Žè¯¦ç»†è§£ç­”ã€‚"
keywords: ["${kw.keyword_zh}", "${kw.keyword}", "AIå¸¸è§é—®é¢˜"]
date: ${date}
tier: 3
lang: zh
type: faq
tags: ["å¸¸è§é—®é¢˜", "AI"]
---

# ${kw.keyword_zh}ï¼šå¸¸è§é—®é¢˜è§£ç­”

ç”Ÿæˆ10ä¸ªé—®ç­”å¯¹ã€‚è¦æ±‚ï¼š
- é—®é¢˜è¦åœºæ™¯åŒ–ï¼ˆçœŸå®žç”¨æˆ·ä¼šæœç´¢çš„é—®é¢˜ï¼‰
- æ¯ä¸ªç­”æ¡ˆ100-300å­—
- ç”¨ H3 (###) æ ‡è®°æ¯ä¸ªé—®é¢˜
- åŒ…å«å®žé™…æ¡ˆä¾‹
- ç»“åˆæœ€æ–°æ–°é—»åŠ¨æ€
- ä¸“ä¸šæœ¯è¯­é¦–æ¬¡å‡ºçŽ°æ ‡æ³¨è‹±æ–‡
- ä¸è¦ç¿»è¯‘è…”`;

  const enContent = runClaudePipe(enPrompt);
  const zhContent = runClaudePipe(zhPrompt);

  const enPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `faq-${slug}-en.md`)
    : path.join(CONTENT_DIRS.faq, `${slug}-en.md`);
  const zhPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `faq-${slug}-zh.md`)
    : path.join(CONTENT_DIRS.faq, `${slug}-zh.md`);

  fs.writeFileSync(enPath, extractMarkdown(enContent));
  fs.writeFileSync(zhPath, extractMarkdown(zhContent));
  result.en_path = enPath;
  result.zh_path = zhPath;
}

function generateCompare(kw: ExtractedKeyword, slug: string, date: string, news: string, result: GeneratedContent) {
  log('GENERATE', `âš–ï¸ Compare: ${kw.keyword}`);

  const enPrompt = `Write a comparison article about "${kw.keyword}" for loreai.dev.

## Context from recent news
${news || 'No specific news context.'}

## Format
Output ONLY the complete markdown file with frontmatter.

---
slug: ${slug}
title: "${kw.keyword} â€” Detailed Comparison"
description: "A comprehensive comparison of ${kw.keyword} with benchmarks, features, pricing, and recommendations."
keywords: ["${kw.keyword}", "AI comparison", "AI tools comparison"]
date: ${date}
tier: 2
lang: en
type: compare
tags: ["compare", "AI"]
---

# ${kw.keyword}

Requirements:
- Include a comparison table (markdown table) with key dimensions
- Cover: features, performance, pricing, use cases, limitations
- 800-1500 words
- End with "Who Should Choose What" recommendations
- Reference recent news/benchmarks
- No fluff phrases`;

  const zhPrompt = `ä¸º loreai.dev æ’°å†™"${kw.keyword_zh}"ï¼ˆ${kw.keyword}ï¼‰çš„å¯¹æ¯”æ–‡ç« ã€‚

## æœ€è¿‘ç›¸å…³æ–°é—»
${news || 'æš‚æ— å…·ä½“æ–°é—»ä¸Šä¸‹æ–‡ã€‚'}

## æ ¼å¼è¦æ±‚
åªè¾“å‡ºå®Œæ•´çš„ markdown æ–‡ä»¶ï¼ˆå« frontmatterï¼‰ã€‚

---
slug: ${slug}
title: "${kw.keyword_zh} â€” è¯¦ç»†å¯¹æ¯”åˆ†æž"
description: "${kw.keyword_zh}çš„å…¨é¢å¯¹æ¯”ï¼šæ€§èƒ½ã€åŠŸèƒ½ã€å®šä»·ä¸ŽæŽ¨èã€‚"
keywords: ["${kw.keyword_zh}", "${kw.keyword}", "AIå¯¹æ¯”"]
date: ${date}
tier: 2
lang: zh
type: compare
tags: ["å¯¹æ¯”", "AI"]
---

# ${kw.keyword_zh}

è¦æ±‚ï¼š
- åŒ…å«å¯¹æ¯”è¡¨æ ¼ï¼ˆmarkdown è¡¨æ ¼ï¼‰
- è¦†ç›–ï¼šåŠŸèƒ½ã€æ€§èƒ½ã€å®šä»·ã€ä½¿ç”¨åœºæ™¯ã€å±€é™æ€§
- 800-1500å­—
- ä»¥"å¦‚ä½•é€‰æ‹©"æŽ¨èç»“å°¾
- å¼•ç”¨æœ€æ–°æ–°é—»/åŸºå‡†æµ‹è¯•
- ä¸“ä¸šæœ¯è¯­é¦–æ¬¡å‡ºçŽ°æ ‡æ³¨è‹±æ–‡`;

  const enContent = runClaudePipe(enPrompt);
  const zhContent = runClaudePipe(zhPrompt);

  const enPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `compare-${slug}-en.md`)
    : path.join(CONTENT_DIRS.compare, `${slug}-en.md`);
  const zhPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `compare-${slug}-zh.md`)
    : path.join(CONTENT_DIRS.compare, `${slug}-zh.md`);

  fs.writeFileSync(enPath, extractMarkdown(enContent));
  fs.writeFileSync(zhPath, extractMarkdown(zhContent));
  result.en_path = enPath;
  result.zh_path = zhPath;
}

function generateTier2(kw: ExtractedKeyword, slug: string, date: string, news: string, result: GeneratedContent) {
  log('GENERATE', `ðŸ“Š Tier 2 Analysis: ${kw.keyword}`);

  const enPrompt = `Write a Tier 2 analysis article about "${kw.keyword}" for loreai.dev.

## Context from recent news
${news || 'No specific news context.'}

## Format
Output ONLY the complete markdown file with frontmatter.

---
slug: ${slug}
title: "${kw.keyword} â€” Analysis and Industry Impact"
description: "In-depth analysis of ${kw.keyword}: what happened, why it matters, and what comes next."
keywords: ["${kw.keyword}", "AI analysis", "AI trends"]
date: ${date}
tier: 2
lang: en
type: blog
tags: ["analysis", "AI trends"]
---

# ${kw.keyword}

**TL;DR:** (One sentence summary)

Requirements:
- 1500-2500 words
- Structure: Background â†’ What Happened â†’ Analysis â†’ Impact â†’ What's Next
- Include data points and specific examples
- Reference the news sources
- Professional but accessible tone
- No fluff phrases like "Let's dive in" or "In this article we will explore"`;

  const zhPrompt = `ä¸º loreai.dev æ’°å†™å…³äºŽ"${kw.keyword_zh}"ï¼ˆ${kw.keyword}ï¼‰çš„ Tier 2 åˆ†æžæ–‡ç« ã€‚

## æœ€è¿‘ç›¸å…³æ–°é—»
${news || 'æš‚æ— å…·ä½“æ–°é—»ä¸Šä¸‹æ–‡ã€‚'}

## æ ¼å¼è¦æ±‚
åªè¾“å‡ºå®Œæ•´çš„ markdown æ–‡ä»¶ï¼ˆå« frontmatterï¼‰ã€‚

---
slug: ${slug}
title: "${kw.keyword_zh} â€” æ·±åº¦åˆ†æžä¸Žè¡Œä¸šå½±å“"
description: "æ·±å…¥åˆ†æž${kw.keyword_zh}ï¼šå‘ç”Ÿäº†ä»€ä¹ˆã€ä¸ºä»€ä¹ˆé‡è¦ã€æŽ¥ä¸‹æ¥ä¼šæ€Žæ ·ã€‚"
keywords: ["${kw.keyword_zh}", "${kw.keyword}", "AIåˆ†æž"]
date: ${date}
tier: 2
lang: zh
type: blog
tags: ["æ·±åº¦åˆ†æž", "AIè¶‹åŠ¿"]
---

# ${kw.keyword_zh}

**ä¸€å¥è¯æ€»ç»“ï¼š**ï¼ˆæ ¸å¿ƒç»“è®ºï¼‰

è¦æ±‚ï¼š
- 1500-2500å­—
- ç»“æž„ï¼šèƒŒæ™¯ â†’ å‘ç”Ÿäº†ä»€ä¹ˆ â†’ åˆ†æž â†’ å½±å“ â†’ å±•æœ›
- åŒ…å«æ•°æ®å’Œå…·ä½“æ¡ˆä¾‹
- å¼•ç”¨æ–°é—»æ¥æº
- è¯­æ°”åƒæ‡‚æŠ€æœ¯çš„æœ‹å‹åœ¨ç§‘æ™®
- ä¸“ä¸šæœ¯è¯­é¦–æ¬¡å‡ºçŽ°æ ‡æ³¨è‹±æ–‡
- ä¸è¦ç¿»è¯‘è…”`;

  const enContent = runClaudePipe(enPrompt);
  const zhContent = runClaudePipe(zhPrompt);

  const enPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `tier2-${slug}-en.md`)
    : path.join(CONTENT_DIRS.tier2, `en/${slug}.md`);
  const zhPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `tier2-${slug}-zh.md`)
    : path.join(CONTENT_DIRS.tier3, `zh/${slug}.md`);

  fs.writeFileSync(enPath, extractMarkdown(enContent));
  fs.writeFileSync(zhPath, extractMarkdown(zhContent));
  result.en_path = enPath;
  result.zh_path = zhPath;
}

function generateTier3(kw: ExtractedKeyword, slug: string, date: string, news: string, result: GeneratedContent) {
  log('GENERATE', `âš¡ Tier 3 Quick Read: ${kw.keyword}`);

  const enPrompt = `Write a Tier 3 quick read article about "${kw.keyword}" for loreai.dev.

## Context from recent news
${news || 'No specific news context.'}

## Format
Output ONLY the complete markdown file with frontmatter.

---
slug: ${slug}
title: "${kw.keyword} â€” Quick Guide"
description: "A practical quick guide to ${kw.keyword} for AI developers and teams."
keywords: ["${kw.keyword}", "AI guide", "how to"]
date: ${date}
tier: 3
lang: en
type: blog
tags: ["quick-read", "practical"]
---

# ${kw.keyword}

**TL;DR:** (One sentence)

Requirements:
- 800-1200 words
- Practical and actionable
- Include code examples or step-by-step instructions where relevant
- Reference the news sources
- No fluff`;

  const zhPrompt = `ä¸º loreai.dev æ’°å†™å…³äºŽ"${kw.keyword_zh}"ï¼ˆ${kw.keyword}ï¼‰çš„ Tier 3 å¿«é€Ÿé˜…è¯»æ–‡ç« ã€‚

## æœ€è¿‘ç›¸å…³æ–°é—»
${news || 'æš‚æ— å…·ä½“æ–°é—»ä¸Šä¸‹æ–‡ã€‚'}

## æ ¼å¼è¦æ±‚
åªè¾“å‡ºå®Œæ•´çš„ markdown æ–‡ä»¶ï¼ˆå« frontmatterï¼‰ã€‚

---
slug: ${slug}
title: "${kw.keyword_zh} â€” å¿«é€ŸæŒ‡å—"
description: "${kw.keyword_zh}å®žç”¨å¿«é€ŸæŒ‡å—ï¼Œé¢å‘AIå¼€å‘è€…å’Œå›¢é˜Ÿã€‚"
keywords: ["${kw.keyword_zh}", "${kw.keyword}", "AIæŒ‡å—"]
date: ${date}
tier: 3
lang: zh
type: blog
tags: ["å¿«é€Ÿé˜…è¯»", "å®žç”¨"]
---

# ${kw.keyword_zh}

**ä¸€å¥è¯æ€»ç»“ï¼š**ï¼ˆæ ¸å¿ƒè¦ç‚¹ï¼‰

è¦æ±‚ï¼š
- 800-1200å­—
- å®žç”¨å¯¼å‘ï¼Œå¯æ“ä½œ
- åŒ…å«ä»£ç ç¤ºä¾‹æˆ–æ“ä½œæ­¥éª¤
- å¼•ç”¨æ–°é—»æ¥æº
- ä¸“ä¸šæœ¯è¯­é¦–æ¬¡å‡ºçŽ°æ ‡æ³¨è‹±æ–‡
- ä¸è¦ç¿»è¯‘è…”`;

  const enContent = runClaudePipe(enPrompt);
  const zhContent = runClaudePipe(zhPrompt);

  const enPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `tier3-${slug}-en.md`)
    : path.join(CONTENT_DIRS.tier3, `en/${slug}.md`);
  const zhPath = DRY_RUN
    ? path.join(OUTPUT_DIR, `tier3-${slug}-zh.md`)
    : path.join(CONTENT_DIRS.tier3, `zh/${slug}.md`);

  fs.writeFileSync(enPath, extractMarkdown(enContent));
  fs.writeFileSync(zhPath, extractMarkdown(zhContent));
  result.en_path = enPath;
  result.zh_path = zhPath;
}

// â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function extractMarkdown(raw: string): string {
  // If output has markdown fences, extract content
  const fenceMatch = raw.match(/```(?:markdown|md)?\n([\s\S]*?)```/);
  if (fenceMatch) return fenceMatch[1].trim();
  
  // If output starts with ---, it's already a markdown file
  if (raw.trim().startsWith('---')) return raw.trim();
  
  // Otherwise return as-is
  return raw.trim();
}

function validateFrontmatter(content: string): { valid: boolean; errors: string[] } {
  const errors: string[] = [];
  const fmMatch = content.match(/^---\n([\s\S]*?)\n---/);
  if (!fmMatch) {
    return { valid: false, errors: ['No frontmatter found'] };
  }

  const fm = fmMatch[1];
  const required = ['slug', 'title', 'description', 'date', 'lang'];
  for (const field of required) {
    if (!new RegExp(`^${field}:`, 'm').test(fm)) {
      errors.push(`Missing required field: ${field}`);
    }
  }

  return { valid: errors.length === 0, errors };
}

// â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function main() {
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('  ðŸš€ Track B: SEO Auto-Pipeline');
  console.log(`  Mode: ${DRY_RUN ? 'DRY RUN' : 'PRODUCTION'}`);
  console.log(`  Date: ${TARGET_DATE}`);
  console.log(`  Max items: ${MAX_ITEMS}`);
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  // Ensure output directory
  if (DRY_RUN) {
    fs.mkdirSync(OUTPUT_DIR, { recursive: true });
  }

  // Step 0: Get news items
  log('NEWS', 'Reading news_items from database...');
  const newsItems = getNewsItems();
  if (newsItems.length === 0) {
    log('NEWS', 'âš ï¸ No news items found in last 48h. Exiting.');
    process.exit(0);
  }
  log('NEWS', `Found ${newsItems.length} news items`);

  // Step 1: Extract keywords
  const existingSlugs = getExistingSlugs();
  const existingTitles = getExistingTitles();
  log('DEDUP', `Existing content: ${existingSlugs.size} slugs, ${existingTitles.size} titles`);

  const keywords = await extractKeywords(newsItems, existingSlugs, existingTitles);
  if (keywords.length === 0) {
    log('KEYWORDS', 'âš ï¸ No keywords extracted. Exiting.');
    process.exit(0);
  }

  // Save extracted keywords
  const kwPath = path.join(OUTPUT_DIR, 'extracted-keywords.json');
  fs.mkdirSync(OUTPUT_DIR, { recursive: true });
  fs.writeFileSync(kwPath, JSON.stringify(keywords, null, 2));
  log('KEYWORDS', `Saved to ${kwPath}`);

  // Step 2: Brave Search validation (optional)
  const validatedKeywords = await validateWithBrave(keywords);

  // Step 3: Generate content
  log('GENERATE', `Generating content for ${validatedKeywords.length} keywords...`);
  const results: GeneratedContent[] = [];

  for (const kw of validatedKeywords) {
    const result = generateContent(kw, newsItems);
    results.push(result);

    // Validate frontmatter
    if (result.status === 'success') {
      for (const p of [result.en_path, result.zh_path]) {
        if (p && fs.existsSync(p)) {
          const content = fs.readFileSync(p, 'utf-8');
          const validation = validateFrontmatter(content);
          if (!validation.valid) {
            log('VALIDATE', `âš ï¸ ${path.basename(p)}: ${validation.errors.join(', ')}`);
          }
        }
      }
    }
  }

  // Step 4: Summary report
  const successful = results.filter(r => r.status === 'success');
  const failed = results.filter(r => r.status === 'error');

  const report = `# SEO Pipeline Report â€” ${TARGET_DATE}

## Summary
- **Mode:** ${DRY_RUN ? 'Dry Run' : 'Production'}
- **News items processed:** ${newsItems.length}
- **Keywords extracted:** ${keywords.length}
- **Content generated:** ${successful.length} âœ… / ${failed.length} âŒ

## Generated Content

${successful.map(r => `### ${r.keyword.keyword} (${r.keyword.type})
- **Slug:** ${r.slug}
- **EN:** ${r.en_path}
- **ZH:** ${r.zh_path}
- **Relevance:** ${r.keyword.relevance}/10 | **Newness:** ${r.keyword.newness}/10
- **Context:** ${r.keyword.context}
`).join('\n')}

${failed.length > 0 ? `## Errors\n\n${failed.map(r => `- **${r.keyword.keyword}:** ${r.error}`).join('\n')}` : ''}

## Keywords Considered

${keywords.map(k => `- [${k.type}] ${k.keyword} (${k.keyword_zh}) â€” rel:${k.relevance} new:${k.newness}`).join('\n')}
`;

  const reportPath = path.join(OUTPUT_DIR, `report-${TARGET_DATE}.md`);
  fs.writeFileSync(reportPath, report);

  console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log(`  âœ… SEO Pipeline Complete`);
  console.log(`  Generated: ${successful.length} content pieces`);
  console.log(`  Errors: ${failed.length}`);
  console.log(`  Report: ${reportPath}`);
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
}

main().catch(e => {
  console.error('Fatal error:', e);
  process.exit(1);
});
