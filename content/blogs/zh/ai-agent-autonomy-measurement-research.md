---
slug: ai-agent-autonomy-measurement-research
title: "AI 智能体自主性度量研究 — 深度分析与行业影响"
description: "深入分析AI 智能体自主性度量研究：发生了什么、为什么重要、接下来会怎样。"
keywords: ["AI 智能体自主性度量研究", "AI agent autonomy measurement research", "AI分析", "Claude Code", "AI安全"]
date: 2026-02-18
tier: 2
lang: zh
type: blog
tags: ["深度分析", "AI趋势"]
---

# AI 智能体自主性度量研究

**一句话总结：** Anthropic 首次系统量化了人类实际授予 AI 智能体的自主权限，发现现实部署远比实验室假设更激进——这为整个行业的安全框架敲响了警钟。

## 当 AI 开始"自己做决定"

2026 年初，AI 领域最热门的词是"智能体"（Agent）。从写代码到管理日程，AI 正在从"回答问题的工具"变成"自主行动的助手"。

但这里有一个根本性问题：**我们到底给了 AI 多大的自主权？这种授权有没有边界？**

以前，这个问题停留在理论讨论层面。安全研究者会假设一些场景，然后推演风险。但假设毕竟是假设——没人知道真实世界里的情况是什么样。

2026 年 2 月 18 日，Anthropic 发布了一项研究，第一次用真实数据回答了这个问题。他们分析了数百万次 Claude Code 和 API 的交互记录，绘制出了迄今为止最清晰的 AI 智能体自主性图景。

结论既让人兴奋，也让人警醒。

## 研究发现了什么？

### 发现一：自主权授予的谱系

Anthropic 的研究首先建立了一个"自主性光谱"（Autonomy Spectrum）的框架。这不是简单的"有没有自主权"的二元划分，而是一个连续的分级体系：

**Level 0 — 纯响应模式**：AI 只回答问题，不执行任何操作。传统聊天机器人属于这一级。

**Level 1 — 受限执行**：AI 可以执行预定义的操作，但每一步都需要人类确认。比如"帮我搜索这个关键词"。

**Level 2 — 会话内自主**：AI 可以在单次会话内自主决策，包括调用工具、执行多步操作，但不能跨会话持久化或影响外部系统。

**Level 3 — 有限系统访问**：AI 可以访问文件系统、运行代码、调用 API，但在沙箱或权限限制下运行。

**Level 4 — 广泛系统访问**：AI 可以进行网络请求、访问数据库、修改生产环境，只有少量硬性限制。

**Level 5 — 完全自主**：AI 可以自主设定目标、分配资源、长期运行，人类只做事后审查。

这个框架本身就很有价值——它让"自主性"从一个模糊的概念变成了可量化的维度。

### 发现二：现实部署比预期更激进

研究的核心发现是：**在真实使用中，人们授予 AI 的自主权远超过安全研究者的保守假设。**

具体数据令人印象深刻：

- 在 Claude Code 的使用场景中，超过 **40%** 的会话达到了 Level 3 或更高——也就是说，AI 被授权访问文件系统和执行代码
- API 用户中，约 **15%** 的部署场景达到了 Level 4，AI 可以进行网络请求和外部系统交互
- 即使是非技术用户，也有相当比例在探索高自主性模式

这意味着什么？安全研究者一直假设"人类会保持在控制回路中"（human-in-the-loop），但实际情况是，**人们迫不及待地想让 AI 独立干活**。

### 发现三：自主权与风险的非线性关系

研究中最微妙的发现是：**风险不是随自主权线性增长的——存在几个关键跃升点。**

从 Level 1 到 Level 2 的跃升相对平滑。AI 在会话内多执行几步操作，风险增量有限。

但从 Level 2 到 Level 3（AI 开始访问文件系统和执行代码）是第一个风险跃升点。一旦 AI 可以运行代码，理论上它就能做任何代码能做的事——包括恶意行为。

从 Level 3 到 Level 4（AI 开始进行网络访问）是第二个跃升点。网络访问意味着 AI 的行为可以影响到本地机器之外的系统。一个配置错误的 API 调用，可能就影响了生产环境。

这个发现的实际意义是：**我们需要在这些跃升点设置更强的防护机制**，而不是对所有级别一视同仁。

### 发现四：不同场景的自主性模式差异巨大

研究还揭示了不同使用场景下自主性模式的巨大差异：

**编程场景**（Claude Code）：自主性授予最高。开发者习惯于让 AI 直接修改代码、运行测试、操作文件系统。这里的信任度很高，但风险也相应集中。

**企业 API 集成**：自主性呈双峰分布。要么是非常保守的只读操作，要么是相当激进的系统集成。中间地带很少。

**个人助理场景**：自主性相对保守，但正在快速增长。用户越来越愿意让 AI 访问日历、邮件、文件。

**内容创作场景**：自主性需求最低，主要还是"问答"模式。

这种差异说明：**"一刀切"的安全策略不可行——需要针对不同场景设计不同的自主性框架。**

## 这项研究为什么重要？

### 填补了理论与现实之间的鸿沟

此前的 AI 安全研究大多基于假设场景。研究者会说"如果 AI 被授予这种权限，可能会出现那种风险"。但没人知道现实中的授权模式是什么样的。

Anthropic 的研究第一次用数据说话。它不是在猜测人们"可能会"怎么用 AI，而是在报告人们"实际正在"怎么用 AI。

这让整个安全讨论有了坚实的基础。

### 为监管提供了量化依据

各国政府都在探索 AI 监管，但面临一个困境：如何定义"高风险 AI 应用"？

Anthropic 的自主性光谱框架提供了一个可能的答案。监管可以不再泛泛地说"AI 系统"，而是具体地说"Level 4 及以上的自主性部署"——这就有了可操作性。

### 指出了安全研究的优先方向

研究中的"风险跃升点"发现，直接指出了安全研究应该聚焦的地方：

- Level 2→3 的过渡（代码执行边界）需要更好的沙箱技术
- Level 3→4 的过渡（网络访问边界）需要更精细的权限控制
- 跨会话持久化和目标设定（通向 Level 5）需要新的监控范式

这比"AI 安全很重要"的泛泛呼吁有价值得多。

## 对不同群体的影响

### 对开发者

如果你正在构建 AI 应用，这项研究给出了几个实际指导：

**明确定义你的自主性级别**。在设计阶段就决定你的应用在光谱上的位置，而不是让它"自然演化"。

**在跃升点设置硬限制**。如果你的应用只需要 Level 2 的能力，就不要"以备将来之需"地开放 Level 3 的权限。

**监控自主性漂移**。用户行为可能会逐渐推动你的应用向更高自主性发展。定期审计实际使用模式，确保它还在你的设计范围内。

### 对企业用户

如果你正在企业环境中部署 AI 智能体：

**审计你的 AI 部署**。按照 Anthropic 的光谱框架，评估你的各个 AI 应用处于哪个级别。很多企业可能会发现，他们的部署比自己以为的更激进。

**分级管理**。不同级别的部署需要不同的审批流程、监控强度和回滚机制。

**培训团队**。确保使用 AI 工具的员工理解他们在授予什么权限。很多人只是点"确定"而不理解背后的含义。

### 对安全研究者

这项研究开辟了新的研究方向：

**跃升点防护**：如何在 Level 2→3、Level 3→4 的过渡处建立更好的防护？
**自主性监控**：如何实时检测 AI 的行为是否超出授权范围？
**可撤销授权**：如何设计"可撤销"的自主权，让人类能在任何时候收回控制？

## 行业的回应

### 其他厂商的跟进

Anthropic 发布这项研究后，预计其他主要 AI 厂商会做出回应：

- OpenAI 可能会发布类似的分析，覆盖 GPT 系列在 Codex 等场景的自主性数据
- Google 可能会为 Gemini 的智能体功能引入类似的分级框架
- Microsoft 可能会在 Copilot 产品线中采用这套术语和框架

这是好事——行业需要共同的语言来讨论自主性问题。

### 开源社区的反应

开源社区的反应可能会更复杂。一方面，自主性光谱的透明度是受欢迎的。另一方面，一些开发者可能会担心这成为限制 AI 能力的借口。

可以预期会出现一些争论：**"高自主性本身是风险"这个前提是否成立？还是说只有"不受监控的高自主性"才是问题？**

## 展望：自主性的未来

### 短期（6-12 个月）

- 主要 AI 厂商会采用类似的自主性分级框架
- API 和应用会开始显式标注自主性级别
- 安全审计开始包含"自主性评估"维度

### 中期（1-2 年）

- 监管框架可能开始引用自主性级别作为合规要求
- 保险公司可能根据自主性级别调整 AI 相关保费
- "自主性认证"可能成为企业 AI 部署的新要求

### 长期思考

最根本的问题是：**人类与 AI 的关系应该是什么样的？**

这项研究显示，人们天然倾向于授予 AI 更多自主权。这是因为自主性带来效率——让 AI 独立干活，人类就可以去做其他事。

但效率不是唯一的考量。自主性的另一面是控制的丧失。当 AI 可以独立行动时，它的行为是否仍然代表我们的意图？

Anthropic 的这项研究没有回答这个哲学问题，但它提供了一个重要的起点：**首先，我们需要知道现实是什么样的；然后，我们才能讨论它应该是什么样的。**

数据显示现实比我们预期的更激进。接下来怎么办，取决于整个行业——和整个社会——的选择。

---

## 常见问题

### 什么是 AI 智能体自主性？

AI 智能体自主性（AI Agent Autonomy）指的是 AI 系统在多大程度上可以独立做出决策和执行操作，而无需人类实时监督或逐步确认。Anthropic 的研究将其分为 6 个级别（Level 0-5），从纯响应模式到完全自主。

### 这项研究的数据来源是什么？

Anthropic 分析了 Claude Code 和 Claude API 的数百万次交互记录。这是首次有 AI 厂商公开分享大规模真实使用场景下的自主性数据，而不是基于假设场景的推演。

### 高自主性就意味着高风险吗？

不完全是线性关系。研究发现存在"风险跃升点"：从会话内操作到代码执行（Level 2→3）、从本地操作到网络访问（Level 3→4）是两个关键跃升点。在这些点上，风险会有显著增加。但在同一级别内部，风险增长相对平缓。

## 参考来源

- [Measuring AI agent autonomy in practice](https://www.anthropic.com/research/measuring-ai-agent-autonomy) — Anthropic Research, 2026-02-18