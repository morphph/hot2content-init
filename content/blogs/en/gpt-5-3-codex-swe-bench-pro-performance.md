---
slug: gpt-5-3-codex-swe-bench-pro-performance
title: "GPT-5.3 Codex Crushes SWE-bench Pro: What You Need to Know"
date: 2026-02-09
lang: en
tier: 3
tags: []
description: "A quick look at GPT-5.3 Codex's impressive SWE-bench Pro performance and other key features, released alongside Anthropic's Claude Opus 4.6."
keywords:
  - "GPT-5.3 Codex SWE-bench Pro"
  - "GPT-5.3 Codex benchmark scores"
  - "SWE-bench Pro performance 2026"
  - "OpenAI Codex coding model"
hreflang_zh: /zh/blog/gpt-5-3-codex-swe-bench-pro-performance
---

OpenAI just dropped GPT-5.3 Codex, a coding-specialized model, and it's making waves. Released alongside Anthropic's Claude Opus 4.6, Codex is boasting some impressive benchmark scores, particularly on SWE-bench Pro. Here's a quick rundown of what you need to know.

## GPT-5.3 Codex: The Highlights

This new iteration of Codex isn't just faster (25% faster than GPT-5.2 Codex!). It's also being touted as a model that helped create itself, instrumental in debugging its own training and managing deployment. Beyond just writing code, GPT-5.3 Codex now supports the full software lifecycle, from debugging and deployment to user research and metric analysis. The Codex Mac app serves as a command center for managing multiple agents.

## SWE-bench Pro Performance and Beyond

The real headline here is the SWE-bench Pro performance: a score of **56.8%**, according to OpenAI. This, along with a reported Terminal-Bench 2.0 score of 77.3% (note: the public Terminal-Bench leaderboard shows top Codex entries around 75%), represents new industry highs. Other notable benchmarks include 80.0% on SWE-Bench Verified and 64.7% on OSWorld-Verified. OpenAI is even classifying this model as "high capability" for cybersecurity tasks.

**Key Takeaways:**

*   GPT-5.3 Codex achieves a leading 56.8% on SWE-bench Pro.
*   Supports the entire software lifecycle, not just coding.
*   First model OpenAI classifies as "high capability" for cybersecurity.
*   Available via ChatGPT Plus and the Codex Mac app.

GPT-5.3 Codex represents a significant step toward AI models that participate in their own development lifecycle. While the SWE-bench Pro score of 56.8% is an incremental improvement over GPT-5.2 Codex's 56.4%, the bigger story is the expansion beyond pure coding into full lifecycle automation â€” from PRDs to deployment. For developers evaluating coding models in 2026, the choice increasingly depends on whether you need terminal-focused automation (Codex's strength) or deep reasoning across large codebases (where Claude Opus 4.6 currently leads).
