# AI Daily Digest â€” Feb 16, 2026

## ğŸ§  MODEL â€” New Releases & Trending Open-Source

- **GLM-5 by Zhipu AI** â€” Most trending model on HuggingFace (1,241 â¤ï¸). New generation from the GLM family.
- **Kimi-K2.5 by Moonshot AI** â€” Topping HF with 2,205 â¤ï¸. Moonshot's latest iteration continues gaining traction.
- **MiniMax-M2.5** â€” 663 â¤ï¸ on HF. MiniMax pushing forward with their M-series.
- **Qwen3.5-397B-A17B & Qwen3-Coder-Next** â€” Qwen continues expanding: massive 397B MoE model and a coding-focused variant (885 â¤ï¸).
- **MiniCPM-o-4.5 & MiniCPM-SALA** by OpenBMB â€” Compact multimodal models (851 â¤ï¸ and 450 â¤ï¸ respectively).
- **Gemini 3 Deep Think** (Google DeepMind, Feb 2026) â€” Advancing science, research and engineering with deeper reasoning capabilities.

## ğŸ“± APP â€” Consumer Updates

- **Claude Code on the Web gets superpowers** â€” Anthropic expanding Claude Code's web experience. Weekly active users doubled since January.
- **Spotify engineering now fully AI-assisted** â€” "Their best developers haven't written a line of code since December" using Claude Code.
- **Claude Code SSH support** â€” Connect to remote machines directly from Claude Code Desktop.

## ğŸ”§ DEV â€” Developer Tools, APIs, SDKs

- **OpenAI Skills in Responses API** (Feb 10) â€” New Skills support for both local and hosted container-based execution. Plus a new Hosted Shell tool with networking.
- **OpenAI Codex CLI** â€” Open-source local coding agent turning natural language into working code, announced by OpenAI Devs.
- **GPT-5.2 & GPT-5.2-Codex now 40% faster** (Feb 3) â€” Optimized inference stack, same model weights.
- **GPT Image Batch API** (Feb 10) â€” Batch support for gpt-image-1.5, gpt-image-1, gpt-image-1-mini.
- **Transformers.js v4 Preview** (HF, Feb 9) â€” Now available on NPM.
- **Daggr by Gradio** (Jan 29) â€” Chain apps programmatically, inspect visually. 98 â¤ï¸ on HF blog.

## ğŸ“ TECHNIQUE â€” Coding Practices & Prompting Tips

- **Infrastructure noise in agentic coding evals** (Anthropic Engineering, featured) â€” Infrastructure configuration can swing agentic benchmarks by several percentage pointsâ€”sometimes more than the gap between top models. Critical reading for anyone running evals.
- **Custom CUDA Kernels from Codex and Claude** (HF, Feb 13) â€” Teaching AI agents to write custom CUDA kernels. Related: "We Got Claude to Build CUDA Kernels and teach open models!" (138 â¤ï¸).
- **OpenEnv: Evaluating Tool-Using Agents** (HF, Feb 12) â€” Real-world environment evaluation for tool-using agents.
- **How Anthropic's marketing team uses Claude Code** â€” Non-eng teams adopting coding agents for automation.

## ğŸš€ PRODUCT â€” New Products & Agents

- **DeerFlow** â€” Multi-agent "superagent" harness gaining attention on X.
- **Memvid** â€” AI agent memory layer using video as a storage medium. Novel approach trending.
- **Project Genie** (Google DeepMind, Jan 2026) â€” Experimenting with infinite, interactive world generation.
- **Veo 3.1 Ingredients to Video** (Google DeepMind) â€” More consistency, creativity and control in video generation.

---

## ğŸ“ MODEL LITERACY: Skills & Agent Specifications

**What are "Skills" in coding agents?** Both OpenAI (Codex) and Anthropic (Claude Code) now support "Skills" â€” reusable, shareable instruction sets that tell a coding agent *how* to perform specific tasks. Think of them like plugins or recipes: instead of explaining your project setup every time, you write a SKILL.md once, and the agent follows it. OpenAI just launched Skills in their Responses API (Feb 10), supporting both local and hosted execution. This mirrors Claude Code's existing AgentSkills system. The convergence means: portable expertise that works across agents is becoming a standard pattern.

---

## ğŸ¯ PICK OF THE DAY: Infrastructure Noise in Agentic Evals

Anthropic's engineering team published a deep investigation showing that **infrastructure configuration alone can swing benchmark results by several percentage points** â€” sometimes more than the actual gap between top models on leaderboards. This matters because the industry is making billion-dollar decisions based on these benchmarks. If your Docker config or network timeout settings affect scores more than model quality, what are we actually measuring? Essential reading for anyone building, evaluating, or buying AI coding tools.

ğŸ‘‰ https://www.anthropic.com/engineering/infrastructure-noise
