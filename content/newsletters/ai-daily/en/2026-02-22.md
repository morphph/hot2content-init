# GGML Joins Hugging Face as OpenAI and Google Race on Enterprise AI

**February 22, 2026**

Today's AI landscape shifted on two fronts. First, the **open-source infrastructure** front: ggml.ai ‚Äî the engine powering nearly every local LLM tool you've used ‚Äî is joining Hugging Face, giving the local AI ecosystem its strongest institutional backing yet. Second, the **enterprise AI** front: OpenAI launched a full agent platform for businesses, Google dropped Gemini 3.1 Pro with doubled reasoning benchmarks, and the agentic coding wars escalated with Claude Code worktrees and GPT-5.3-Codex-Spark. Here's the full breakdown.

---

## üß† MODEL

**GGML joins Hugging Face.** Georgi Gerganov, the creator of llama.cpp ‚Äî the backbone of nearly every local LLM deployment including Ollama, LM Studio, Jan, and dozens more ‚Äî is joining Hugging Face. This isn't just a talent acquisition. llama.cpp processes billions of local AI queries daily, and this move means better resources, faster development cycles, and deeper integration with the world's largest model hub. Easily the most consequential open-source AI infrastructure move this quarter. [Read more ‚Üí](https://github.com/ggml-org/llama.cpp/discussions/19759)

**Gemini 3.1 Pro launches.** Google DeepMind's new flagship model scores more than double its predecessor on ARC-AGI-2 ‚Äî the benchmark designed to test genuine reasoning ability rather than pattern matching. This is a significant leap, and it's rolling out across both the Gemini App and NotebookLM simultaneously. The complex reasoning gap between Google and the competition just narrowed considerably. [Read more ‚Üí](https://x.com/GoogleDeepMind/status/2024516464892334129)

**Consistency diffusion language models.** Together AI published research demonstrating up to 14x faster LLM generation with no quality loss. The approach fundamentally rethinks how language models generate text, moving from sequential token-by-token to parallel generation. If this scales, it could reshape the economics of LLM inference ‚Äî cheaper, faster, and more accessible for everyone. [Read more ‚Üí](https://www.together.ai/blog/consistency-diffusion-language-models)

---

## üì± APP

**OpenAI Frontier ‚Äî enterprise agent platform.** OpenAI's biggest enterprise play yet: a new platform for building, deploying, and managing "AI coworkers that do real work." This isn't just an API ‚Äî it's a full orchestration layer for enterprise agents, complete with deployment management and monitoring. The message is clear: OpenAI wants to own the enterprise AI agent stack. [Read more ‚Üí](https://x.com/OpenAI/status/2019413712772411528)

**Ads coming to ChatGPT.** OpenAI has started rolling out ads to free and Go tier users. The timing is notable ‚Äî Anthropic's most-liked tweet this week was a pointed: "Ads are coming to AI. But not to Claude." The AI business model war is now fully in the open: OpenAI bets on scale + ads, Anthropic bets on premium subscriptions and trust. [Read more ‚Üí](https://x.com/OpenAI/status/2020936703763153010)

---

## üîß DEV

**Claude Code gets built-in git worktree support.** This is a bigger deal than it sounds. Multiple AI agents can now each check out a separate working copy of the same repo and work on parallel branches independently ‚Äî no merge conflicts, no stepping on each other's changes. For teams running multiple coding agents, this unlocks truly parallel development workflows that were previously impossible. [Read more ‚Üí](https://x.com/bcherny/status/2025007393290272904)

**GPT-5.3-Codex-Spark.** OpenAI's new ultra-fast model purpose-built for real-time coding assistance, available in research preview for ChatGPT Pro users. With 128K context and optimized latency, it's designed for the interactive coding loop ‚Äî think autocomplete on steroids, not batch code generation. [Read more ‚Üí](https://x.com/OpenAIDevs/status/2022009906329739681)

**Claude Code Desktop upgrades.** Server preview, automatic code review, and auto-fix + merge PR capabilities all in one release. The vision is becoming clear: a single tool that handles the entire code-to-deploy loop ‚Äî write, review, fix, merge, ship. [Read more ‚Üí](https://x.com/adocomplete/status/2024933963186495499)

**Anthropic API auto-caches prompts.** No more manual cache breakpoints ‚Äî Anthropic now automatically identifies and caches repeated prompt segments. For developers, this means lower costs without any configuration changes. A small but meaningful quality-of-life improvement that reduces the friction of building on Claude. [Read more ‚Üí](https://x.com/alexalbert__/status/2024586006633271386)

**Ollama supports subagents + web search in Claude Code.** You can now run Ollama models as subagents with web search capability inside Claude Code ‚Äî no MCP server setup, no API keys required. The barrier to local AI-assisted development just dropped to near zero. [Read more ‚Üí](https://x.com/ollama/status/2023645972778168576)

**Figma √ó Claude Code.** A new integration lets you bring UI work done in Claude Code directly into Figma as editable design frames. This bridges the gap between AI-generated code and design tools ‚Äî designers can now iterate on AI-built interfaces without starting from scratch. [Read more ‚Üí](https://x.com/trq212/status/2023797194017706290)

**Open-sourced: 30 parallel AI coding agents manager.** A production system for orchestrating 30 parallel coding agents per repo. The tooling around multi-agent development is maturing fast ‚Äî we're moving from "one AI assistant" to "a team of AI developers." [Read more ‚Üí](https://x.com/agent_wrapper/status/2024885035774738700)

---

## üìù TECHNIQUE

**Karpathy: "Claws are now a new layer on top of LLM agents."** Andrej Karpathy coined a new term for the wrapper tooling layer evolving around base model agents. The insight: as models become commoditized, the value shifts to the orchestration and tooling layer that makes them useful in real workflows. Worth watching how this framing shapes the next wave of AI tooling. [Read more ‚Üí](https://twitter.com/karpathy/status/2024987174077432126)

**Anthropic study: AI coding may reduce skill mastery.** A nuanced finding ‚Äî the effect depends heavily on usage patterns. Developers who use AI as a crutch see skill degradation; those who use it as a learning accelerator don't. Important reading for any team rolling out AI coding tools, with practical guidance on healthy adoption patterns. [Read more ‚Üí](https://x.com/AnthropicAI/status/2016960382968136138)

**The path to ubiquitous AI (17k tokens/sec).** A deep technical dive on achieving extreme throughput for AI inference. The thesis: when you can serve 17,000 tokens per second per user, entirely new application categories become viable. Detailed engineering breakdown of what it takes to get there. [Read more ‚Üí](https://taalas.com/the-path-to-ubiquitous-ai/)

---

## üöÄ PRODUCT

**NVIDIA deploys Codex company-wide to ~30,000 engineers.** The largest known enterprise AI coding agent deployment. When the company that builds the chips powering AI goes all-in on AI coding tools internally, it's both a product endorsement and a signal about where software engineering is heading. [Read more ‚Üí](https://x.com/OpenAIDevs/status/2021742142838993059)

**GPT-5.2 derives new result in theoretical physics.** In collaboration with researchers at the Institute for Advanced Study, Harvard, and Cambridge, GPT-5.2 contributed to a preprint with novel findings in theoretical physics. We're past the "AI summarizes papers" phase ‚Äî this is AI participating in original research. [Read more ‚Üí](https://x.com/OpenAI/status/2022390096625078389)

---

## üéì MODEL LITERACY: Git Worktrees for AI Agents

When Claude Code gets "git worktree support," it means multiple AI agents can each check out a separate working copy of the same repo simultaneously. Think of it like giving each agent their own desk with their own copy of the codebase ‚Äî they can all work in parallel without stepping on each other's changes. When done, their work merges back together. This is the infrastructure that enables "teams of AI agents" working on large codebases ‚Äî and it's now built into the tool.

---

## üéØ PICK OF THE DAY: GGML Joins Hugging Face

Why this matters more than any model release this week: llama.cpp processes billions of local AI queries daily. It's the invisible infrastructure behind Ollama, LM Studio, Jan, and dozens of other tools that let you run AI on your own hardware. Georgi Gerganov joining Hugging Face means the most important local AI engine now has institutional backing, dedicated resources, and deeper integration with the largest model hub in the world. For anyone who cares about running AI locally ‚Äî and that's an increasingly large number of developers and companies ‚Äî the foundation just got a lot stronger. [Read more ‚Üí](https://github.com/ggml-org/llama.cpp/discussions/19759)
