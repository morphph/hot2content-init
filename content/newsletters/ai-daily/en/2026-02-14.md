# AI Daily Digest â€” Feb 14, 2026

## ðŸ§  MODEL

**GPT-5.2 derives new physics result** â€” OpenAI announced GPT-5.2 derived a novel result in theoretical physics, publishing a preprint with researchers from IAS, Vanderbilt, and Cambridge. The "AI can't do novel science" â†’ "of course AI does novel science" shift is happening fast. (8.8K likes)

**GLM-5 drops on HuggingFace** â€” Zhipu AI's GLM-5 is trending hard (1.1K likes, 66K downloads). Also notable: MiniMax-M2.5, MiniCPM-SALA from OpenBMB, and Qwen3-Coder-Next (858 likes, 249K downloads) â€” Chinese open-source AI continues to ship fast.

**Gemini 3 Deep Think upgrade** â€” Google DeepMind's Gemini 3 Deep Think gets an upgrade for complex modeling and science tasks. Also: Project Genie for generating interactive worlds.

**Kimi-K2.5 still dominating** â€” Moonshot AI's multimodal model leads HF trending with 2.1K likes and 725K downloads.

## ðŸ“± APP

**Claude Code 2.1.41 ships** â€” Ado notes "lots of good stuff" but also removed a key productivity hack (launching Claude from terminal). Felix Rieseberg says "Don't sleep on Claude Code in the desktop app." (762 + 272 likes)

**OpenAI Codex-Spark** â€” OpenAI's community buzzing about Codex-Spark being fast. Also: GPT-5.2 and GPT-5.2-Codex now run ~40% faster via inference stack optimization.

**Seedance 2.0 drama** â€” Ethan Mollick notes "Hollywood is dead" clips are mostly remixes of existing Hollywood movies. Levelsio calls out AI startups lying about being Seedance 2.0 launch partners.

## ðŸ”§ DEV

**OpenAI launches Skills in Responses API** â€” New feature lets you add Skills (agent capabilities) to API responses. Also launched Hosted Shell tool with networking support in containers. (Feb 10)

**Custom CUDA Kernels via Codex/Claude** â€” HuggingFace blog: agents can now write custom CUDA kernels. Previously, this was elite GPU programmer territory.

**300+ MCP servers open-sourced** â€” Someone collected 300+ MCP servers in one repo, trending hard on X (3.6K likes). MCP ecosystem continues rapid expansion.

**Anthropic partners with CodePath** â€” Bringing Claude and Claude Code to 20,000+ computer science students at colleges. (1.9K likes)

## ðŸ“ TECHNIQUE

**Spotify ships with Claude Code** â€” Boris Cherny: "Their best developers haven't written a single line of code since December, they fix bugs from [phone]." Major signal for vibe coding going mainstream. (4.1K likes)

**Anthropic: Infrastructure noise in coding evals** â€” New engineering blog post: infrastructure config can swing agentic coding benchmarks by several percentage pointsâ€”sometimes more than the leaderboard gap between top models. Important for anyone comparing AI coding tools.

**Premium UI/UX prompt** â€” Trending prompt that turns your AI coding agent into a "premium UI/UX architect with Steve Jobs and Jony Ive's design philosophies." (4.4K likes)

## ðŸš€ PRODUCT

**OpenAI Batch API for image models** â€” GPT-image-1.5, gpt-image-1, gpt-image-1-mini now support batch processing. Good for bulk image generation workflows.

**Open Responses spec** â€” OpenAI released an open-source spec for building multi-provider, interoperable LLM interfaces. Cross-provider compatibility getting easier.

**Transformers.js v4 Preview** â€” Now on NPM. Run transformer models directly in the browser/Node.js. (63 likes on HF blog)

---

## ðŸŽ“ MODEL LITERACY: Infrastructure Noise in Benchmarks

When you see "Model A scores 82% on SWE-bench and Model B scores 80%", that 2% gap might not mean anything. Anthropic's latest engineering post shows that just changing infrastructure config (timeout settings, retry logic, container setup) can swing benchmark scores by several percentage points.

**Why it matters:** Don't pick your AI coding tool based on leaderboard rankings alone. The gap between top models is often smaller than the noise from how the test was run. Try them yourself on YOUR codebase.

---

## ðŸŽ¯ PICK OF THE DAY: Spotify's Claude Code Workflow

Boris Cherny's tweet about Spotify developers not writing code since December is the most important signal this week. This isn't a demo or a hackathon project â€” it's a major tech company's production workflow.

The pattern: senior devs define what needs to happen, Claude Code executes, devs review and ship. Sound familiar? It's exactly the "product thinking > coding ability" thesis. If Spotify's best engineers are working this way, the question isn't whether vibe coding works â€” it's how fast everyone else adopts it.

**Read more:** @bcherny's thread + his appearance on @garrytan's podcast.
