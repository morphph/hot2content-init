---
slug: claude-code-and-what-comes-next-by-ethan-mollick
title: "Claude Code and What Comes Next - by Ethan Mollick"
description: "Expert answers to Claude Code and What Comes Next - by Ethan Mollick and related questions about Claude Code."
keywords: ["Claude Code", "Claude Code and What Comes Next", "AI coding agents", "Ethan Mollick", "AI startups"]
date: 2026-02-20
tier: 3
lang: en
type: faq
tags: ["faq", "AI", "Claude Code"]
---

# Claude Code and What Comes Next - by Ethan Mollick

## What is the Ethan Mollick "Claude Code" experiment about?

Ethan Mollick, a Wharton professor known for his AI research, conducted an experiment where he gave Claude Code a single command: "Develop a web-based or software-based startup idea that will make me $1000 a month where you do all the work by generating the idea and implementing it. I shouldn't have to do anything at all."

The experiment explored whether an AI coding agent could autonomously generate a viable business idea and build it from scratch. According to Mollick's Substack post "One Useful Thing," Claude Code responded by creating a complete product concept and implementing it—demonstrating that AI agents can now handle end-to-end software development tasks that previously required significant human effort.

The broader significance lies in what this capability implies for the future of software creation. If an AI can generate business ideas, write the code, and deploy products with minimal human intervention, the barriers to building software drop dramatically. Mollick's experiment became a touchpoint for discussions about AI-assisted entrepreneurship and the democratization of software development.

However, critics like Dave Karpf have raised concerns about what happens when everyone has access to the same tool. As Karpf noted, "All of the Claude Code instances are clustering around the same set of indistinguishable business ideas," suggesting that mass adoption could lead to market saturation with AI-generated, low-quality products. This tension between accessibility and differentiation represents one of the key challenges in the Claude Code era.

### Can Claude Code really build a complete startup without human coding?

Based on Mollick's experiment, Claude Code can generate business ideas, write functional code, and create deployable applications autonomously. The AI handles the technical implementation—writing backend logic, frontend interfaces, and infrastructure setup—without requiring the user to write code manually.

That said, "complete startup" overstates current capabilities. Claude Code excels at implementation but still requires human judgment for business decisions, market validation, customer acquisition, and ongoing operations. The AI can build the product, but turning that into a sustainable $1,000/month business involves factors beyond code generation.

The experiment demonstrated proof-of-concept: AI agents can compress weeks of development work into hours. But success depends on the idea's viability and the human's ability to market and iterate on what the AI builds.

### What are the risks of everyone using Claude Code for the same business ideas?

Dave Karpf's critique highlights a significant concern: when the same AI tool generates ideas for thousands of users, clustering occurs. Claude Code instances tend to converge on similar concepts because they draw from overlapping training data and optimization patterns.

The predicted outcome is market flooding. Karpf warns that "the Internet is awash in AI-generated, low-quality product offerings" as scammers, spammers, and what he calls "low-effort hustlebros" all prompt Claude Code for money-making ideas. Ironically, Mollick's own prompt pack business could stop generating revenue because competitors replicate it instantly using the same tools.

This creates a race-to-the-bottom dynamic where differentiation becomes harder, and AI-generated sameness commoditizes entire product categories.

### What does "agentic coding" mean in this context?

Agentic coding refers to AI systems that operate with autonomy rather than just responding to individual prompts. Claude Code exemplifies this approach—given a high-level goal, it breaks down tasks, makes implementation decisions, and executes multi-step workflows independently.

Unlike traditional AI assistants that answer questions or complete isolated tasks, agentic coders maintain context across an entire project, debug their own errors, and iterate toward functional software. The user provides intent; the agent handles execution.

This shift matters because it changes who can build software. Non-programmers can describe what they want and receive working code, compressing the traditional development cycle from months to minutes for simple applications.

### Is paying for AI-generated prompt packs worth it when anyone can create them?

The comments on Mollick's post raise this directly: some commenters note that "many people might end up paying $39 for a prompt pack that takes Claude a few minutes to create." This represents the arbitrage window that exists during technological transitions.

Currently, value exists because awareness is uneven—not everyone knows they can generate equivalent content themselves. But this window closes as AI literacy spreads. Products built on AI-generated content face commoditization pressure as the underlying capability becomes common knowledge.

For buyers, the practical advice is: before purchasing AI-generated content products, attempt to create equivalent outputs yourself using tools like Claude Code. The cost of experimentation is now measured in minutes, not money.