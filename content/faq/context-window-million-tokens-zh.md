---
title: "百万 Token 上下文窗口"
description: "关于百万 token 上下文窗口的常见问题：如何使用、局限性、最佳实践"
date: 2026-02-10
lang: zh
---

### 什么是 100 万 token 上下文窗口？

100 万 token 上下文窗口意味着语言模型在单次交互中可以处理多达 100 万个 token（约 **75 万个英文单词**，或 **150-200 万个中文字符**）。上下文窗口的演进：

* **GPT-3（2020）**：4K tokens
* **GPT-4（2023）**：8K-128K tokens
* **Claude 3.5（2024）**：200K tokens
* **Claude Opus 4.6（2026）**：**100 万 tokens**（测试版）
* **Gemini 2.5 Pro（2026）**：**200 万 tokens**

实际意义：你可以将整个代码库（5 万行以上）、一套完整的法律合同、或数月的对话历史放入单个提示中。模型可以同时推理所有内容。

---

### 如何有效使用百万 token 上下文窗口？

简单地把 token 塞满窗口并不保证好的结果。最佳实践：

1. **结构化输入**：使用清晰的标题、文件分隔符和标记，让模型能导航内容
2. **关键信息放首尾**：模型对上下文开头和结尾的关注度最高（"中间遗失"问题仍然存在）
3. **使用 XML 标签或分隔符**：`<file path="src/main.ts">...</file>` 帮助模型理解文档边界
4. **明确聚焦点**：说明需要关注哪些部分
5. **结合提示缓存**：缓存静态上下文（代码库），只变化查询——节省高达 90% 的成本

不需要时不要使用全部窗口。小而精的上下文往往比大而散的上下文产生更好的结果。

---

### 大上下文窗口有什么局限性？

尽管规模惊人，百万 token 上下文仍有实际限制：

* **中间遗失**：模型可能忽略放在很长上下文中间的信息。关键细节应放在开头或结尾。
* **成本高**：在 Claude Opus 4.6 上处理 100 万 token 每次请求约 $5 输入 + $25 输出。
* **延迟**：更多 token = 更长的处理时间。
* **边际递减**：超过一定量，添加更多上下文不会改善输出质量，甚至可能降低。
* **非真正"理解"**：模型通过统计方式处理 token，不像人类那样深度理解文档。

对于许多场景，**RAG（检索增强生成）** ——只获取相关片段——优于暴力填充整个上下文。

---

### 百万 token 上下文 vs RAG：该用哪个？

两种方法各有优势：

| | 大上下文窗口 | RAG |
|---|---|---|
| **适合** | 整体分析、交叉引用 | 从海量数据中精确检索 |
| **成本** | 高（处理所有内容） | 较低（只处理相关片段） |
| **准确性** | 可能遗漏中间内容 | 取决于检索质量 |
| **设置** | 简单（直接发送 token） | 需要向量数据库、嵌入、管道 |

**大上下文适合**：需要模型理解文档间关系的场景（代码审查、法律分析、研究综合）。

**RAG 适合**：语料库超过 100 万 token，需要回答特定问题，或对成本/延迟敏感的场景。

许多生产系统将两者结合：RAG 检索最相关的文档，然后放入大上下文窗口进行分析。

---

### 使用完整 100 万上下文的成本是多少？

在 Claude Opus 4.6 上使用完整 100 万 token 上下文：

* **输入**：100 万 token × $5/百万 = **每次请求 $5.00**
* **输出**（最大 128K）：128K × $25/百万 = **每次请求 $3.20**
* **总计**：约 **$8.20/次**

优化策略：提示缓存节省高达 **90%**，批处理节省 **50%**，用小模型先筛选再用 Opus 处理，使用 Compaction API 处理长对话。

---

### 100 万 token 可以装下什么？

实际容量示例：

* **代码**：约 5-7 万行代码（中型应用）
* **文档**：约 75 万英文单词（7-8 本小说）或约 150 万中文字
* **对话**：约 50 万条消息
* **数据**：约 10 万行结构化 CSV 数据

真实应用：一次性审查整个代码仓库、完整的法律文书分析、全面的患者病历审查、多文档研究综合。

---

### 所有 Claude 模型都支持完整上下文窗口吗？

不是，各模型上下文大小不同：

* **Claude Opus 4.6**：100 万 tokens（测试版）——唯一支持百万窗口的模型
* **Claude Sonnet 4.5**：20 万 tokens
* **Claude Haiku 4**：20 万 tokens

100 万上下文目前处于测试阶段，可能有可用性限制。生产系统应计划在百万窗口不可用时优雅降级到 20 万。

---

### Compaction API 与上下文窗口是什么关系？

Compaction API 是大上下文窗口的补充，而非替代。它通过总结旧上下文实现无限对话：

* **上下文窗口**：固定大小的"工作记忆"——所有内容必须同时放入
* **Compaction API**：服务端总结，压缩旧的对话轮次，为新内容腾出空间

这对**长时间运行的代理会话**、聊天机器人和跨越数天或数周的编程工作流非常理想。

---

### 上下文窗口会继续增长吗？

趋势表明会，但实际收益递减：

* 2023 年 10 万是革命性的，2024 年 20 万成为标准，2026 年百万到两百万可用
* 2027 年以后可能达到千万级，但未必有必要

真正的问题不是"多大"，而是"模型用上下文用得多好"。研究重点正在转向更好的注意力机制、更高效的架构、混合方法（RAG + 大上下文），以及用于无限有效上下文的压缩和总结技术。

对大多数开发者而言，实际瓶颈不是上下文窗口大小——而是处理百万 token 的成本和延迟。
