---
term: "幻觉（Hallucination）"
slug: hallucination
lang: zh
category: LLM 基础概念
definition: "AI 模型生成看起来合理、但实际上是错误的、捏造的或缺乏依据的信息——以高度自信的语气将虚假内容当作事实呈现。"
related: [rag-retrieval-augmented-generation, prompt-engineering, fine-tuning]
date: 2026-02-10
source_topic: hallucination
keywords:
  - "AI 幻觉"
  - "模型幻觉"
  - "AI 准确性"
---

## 什么是 AI 幻觉？

AI 幻觉（Hallucination）是指大语言模型生成的内容在事实上是错误的、完全编造的，或者没有任何依据——但模型却以与正确信息同样自信的语气输出这些内容。这个术语借用了人类"幻觉"的概念：模型"看到"了并不存在的东西。

常见的幻觉表现包括：引用不存在的学术论文、编造统计数据、虚构历史事件、生成看似正确但实际有 bug 的代码。最危险的地方在于，这些输出往往语法通顺、术语准确、逻辑连贯，如果不经过验证，很难发现问题。

## 为什么模型会产生幻觉？

幻觉源于 LLM 的基本工作原理：

- **下一个 Token 预测**：模型的训练目标是预测最可能的下一个 Token，而非验证事实真伪。它优化的是"听起来合理"，不是"确实正确"
- **训练数据空白**：当模型遇到训练数据稀少的话题时，会用统计上合理（但编造的）细节来填补空缺
- **讨好倾向**：模型倾向于给出"有帮助"的回答，而不是坦承"我不知道"
- **信息混合**：模型可能将不同来源的信息错误组合，创造出"嵌合体事实"——每个组成部分可能是真的，但组合在一起就是错的
- **缺乏世界模型**：LLM 没有经过验证的知识库可以核对，它完全依赖学到的统计模式

## 如何减少幻觉？

2026 年，AI 行业已经发展出多种应对策略：

- **RAG（检索增强生成）**：让模型基于检索到的真实文档回答问题，而非仅凭"记忆"
- **引用来源**：Claude 等模型可以标注具体引用段落，方便用户核实
- **RLHF 与 Constitutional AI**：训练模型在不确定时主动表达不确定性
- **工具调用**：让模型查询数据库、搜索引擎或计算器，而不是凭空生成事实
- **思维链推理**：逐步推理可以减少逻辑错误和事实捏造
- **自动化评估**：使用事实核查流水线自动验证模型输出

尽管取得了显著进步，幻觉问题仍然是 LLM 在医疗、法律、金融等高风险领域部署时面临的最大挑战之一。

## 相关术语

- **RAG（检索增强生成）**：将模型输出锚定在真实数据上的核心技术
- **Prompt 工程**：精心设计的提示词可以降低幻觉发生率
- **微调（Fine-Tuning）**：针对特定领域的微调可以提高事实准确性
