---
term: "上下文窗口（Context Window）"
slug: context-window
lang: zh
category: LLM 基础
definition: '语言模型在单次交互中能处理的最大 token（词/子词）数量，决定了模型一次能"看到"多少文本。'
related: [agent-teams, adaptive-thinking]
date: 2026-02-09
source_topic: claude-opus-context-window
keywords:
  - "上下文窗口"
  - "token 限制"
  - "AI 上下文长度"
---

## 什么是上下文窗口？

上下文窗口定义了大语言模型（LLM）一次能读取和推理的文本上限。可以把它理解为模型的工作记忆——窗口内的一切都可见，窗口外的则被"遗忘"。

截至 2026 年 2 月，上下文窗口已经大幅扩展。**Claude Opus 4.6** 提供 **100 万 token** 的上下文窗口（测试版，此前为 20 万），而大多数 GPT-5 系列模型运行在 **12.8 万-25.6 万 token**。100 万 token 大约相当于 **7-8 本完整长篇小说**，或一个中等规模的完整代码库。

## 工作原理

当你向 LLM 发送提示时，模型使用自注意力机制处理上下文窗口内的所有 token：

- **输入 token**：你的提示、系统指令和提供的文档
- **输出 token**：模型的回复，逐个 token 生成
- **总预算**：输入 + 输出必须在上下文窗口内
- **注意力计算**：每个 token 都要关注其他所有 token，成本随窗口大小呈二次方增长

更大的上下文窗口使得无需摘要或分块即可处理完整文档、代码库或长对话历史成为可能。

## 为什么重要

上下文窗口大小直接影响 AI 能完成什么：

- **代码审查**：100 万 token 窗口可以容纳整个代码仓库，实现整体分析而非逐文件审查
- **文档分析**：法律合同、研究论文和财务报告可以整体处理
- **长对话**：扩展的来回交互不会丢失之前的上下文

但更大并不总是更好。研究表明模型可能对超长上下文**中间部分**的信息处理不佳（"中间迷失"问题）。此外，处理完整上下文窗口的成本很高。

Anthropic 随 Claude Opus 4.6 推出的 **Compaction API** 提供了另一种方案：服务端上下文摘要，使得对话可以在不触及窗口限制的情况下无限延续。

## 相关术语

- **Token**：LLM 中文本处理的基本单位
- **Compaction API**：用于长对话的服务端上下文摘要
- **RAG（检索增强生成）**：大上下文窗口的替代方案，按需检索相关片段
