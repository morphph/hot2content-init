---
term: "RAG（检索增强生成）"
slug: rag-retrieval-augmented-generation
lang: zh
category: LLM 基础
definition: "一种在生成回答前从外部知识库检索相关信息来增强 LLM 回复的技术，可减少幻觉并实现对最新或私有数据的访问。"
related: [context-window, hallucination]
date: 2026-02-10
source_topic: rag
---

## 什么是 RAG？

RAG（Retrieval-Augmented Generation，检索增强生成）是一种将信息检索与文本生成相结合的架构模式。RAG 不仅依赖模型的训练数据，还从外部知识库获取相关文档并纳入提示，让模型能访问特定的、最新的或私有的信息。

基本流程：**查询 → 检索相关文档 → 用文档增强提示 → 生成回答**

## 工作原理

典型的 RAG 系统包含三个组件：

- **嵌入模型**：将文档和查询转换为向量表示
- **向量数据库**：存储文档嵌入以进行快速相似性搜索（如 Pinecone、Weaviate、pgvector）
- **LLM**：使用检索到的上下文生成回答

用户提问时：(1) 查询被嵌入，(2) 从向量存储中检索相似文档，(3) 检索到的文档插入提示，(4) LLM 基于这些文档生成有根据的回答。

## 为什么重要

RAG 解决了 LLM 的根本局限：

- **知识截止**：模型只知道训练数据中的内容。RAG 提供对最新信息的访问
- **减少幻觉**：将回答建立在检索文档上，显著减少编造的内容
- **私有数据**：RAG 使 LLM 无需微调即可回答关于专有文档的问题
- **成本效率**：比微调或使用大上下文窗口处理大量文档更便宜
- **可追溯**：可以追踪哪些文档支持了每个回答

2026 年，随着上下文窗口达到 100-200 万 token，RAG 与大上下文的关系已经演变。对于小语料库（<100 万 token），直接上下文注入可能优于 RAG。对于更大的集合，RAG 仍然必不可少。许多系统使用混合方法——RAG 检索候选文档，然后大上下文窗口整体处理。

## 相关术语

- **上下文窗口**：RAG 的替代方案——直接将所有相关数据放入提示
- **幻觉**：RAG 通过将回答建立在源文档上来缓解的问题
- **微调**：自定义 LLM 行为的另一种方式，与 RAG 互补
